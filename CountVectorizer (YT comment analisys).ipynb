{"cells":[{"source":"import pandas as pd\nimport nltk\n\nnltk.download('stopwords')\n\n#Load dataset\nytc = pd.read_csv('YoutubeCommentsDataSet.csv')\n\nytc = ytc.dropna().reset_index(drop=True)\nytc.head()","metadata":{"executionCancelledAt":null,"executionTime":58,"lastExecutedAt":1742382535043,"lastExecutedByKernel":"a12c62f9-7958-4dc4-9edd-55951ad98c27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nimport nltk\n\nnltk.download('stopwords')\n\n#Load dataset\nytc = pd.read_csv('YoutubeCommentsDataSet.csv')\n\nytc = ytc.dropna().reset_index(drop=True)\nytc.head()","outputsMetadata":{"0":{"height":60,"type":"stream"},"1":{"height":50,"type":"dataFrame","tableState":{"quickFilterText":"","customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"855c1d40-c4d8-4cf7-b620-fe569fb05f3e","nodeType":"const"}}}}},"cell_type":"code","id":"e39fb2ad-a0ea-4e38-a87f-fc33cbde0fa1","outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Comment","type":"string"},{"name":"Sentiment","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"Comment":["lets not forget that apple pay in 2014 required a brand new iphone in order to use it a significant portion of apples user base wasnt able to use it even if they wanted to as each successive iphone incorporated the technology and older iphones were replaced the number of people who could use the technology increased","here in nz 50 of retailers don’t even have contactless credit card machines like paywave which support apple pay they don’t like the high fees that come with these","i will forever acknowledge this channel with the help of your lessons and ideas explanations now its quite helpful while youll just sit at your comfort and monitor your account growth","whenever i go to a place that doesn’t take apple pay doesn’t happen too often it’s such a drag between ‘contactless covid’ habits and my getting the apple card i’ve gotten so used to apple pay that i get seriously annoyed when a store doesn’t take it it feels like a shock it’s crazy how quickly it took over my shopping routine i’ve officially been brainwashed by apple because now it feels so inconvenient to even carry a physical card in my pocket","apple pay is so convenient secure and easy to use i used it while at the korean and japanese airports no need for physical credit cards"],"Sentiment":["neutral","negative","positive","negative","positive"]}},"total_rows":5,"truncation_type":null},"text/plain":"                                             Comment Sentiment\n0  lets not forget that apple pay in 2014 require...   neutral\n1  here in nz 50 of retailers don’t even have con...  negative\n2  i will forever acknowledge this channel with t...  positive\n3  whenever i go to a place that doesn’t take app...  negative\n4  apple pay is so convenient secure and easy to ...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>lets not forget that apple pay in 2014 require...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>here in nz 50 of retailers don’t even have con...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>i will forever acknowledge this channel with t...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>whenever i go to a place that doesn’t take app...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>apple pay is so convenient secure and easy to ...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":57}],"execution_count":57},{"source":"import pandas as pd\nfrom nltk.corpus import stopwords as nltk_stopwords\nimport spacy\n\n\n# Load the spaCy model\nnlp = spacy.load('en_core_web_sm')\n\n# Preprocessing function\ndef preprocess(text):\n    doc = nlp(text, disable=['ner', 'parser'])\n    tokens = [token for token in doc if token.text.isalpha()]\n    lemmas = [token.lemma_ for token in tokens if token.pos_ not in ['DET', 'NUM', 'SYM', 'X']]\n    #pos = [token.pos_ for token in doc]\n    #print(pos[:10])\n    # Load stopwords from NLTK\n    #stopwords = set(nltk_stopwords.words('english'))\n    # Remove stopwords characters\n    #a_lemmas = [lemma for lemma in lemmas if lemma not in stopwords]\n    \n    return ' '.join(lemmas)\n\n# Preprocessing dataset \nytp = pd.DataFrame(columns=['Comment'])\n\ncomments_list = ytc['Comment'].tolist() \n\nfor i, comment in enumerate(comments_list):\n    if i <= (ytc.shape[0]): #-18000 subtract to limit rows for tests\n        comment = preprocess(comment)\n        ytp = pd.concat([ytp, pd.DataFrame({'Comment': [comment]}, index=[i])])\n\nytp.head()","metadata":{"executionCancelledAt":null,"executionTime":91378,"lastExecutedAt":1742382631442,"lastExecutedByKernel":"a12c62f9-7958-4dc4-9edd-55951ad98c27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom nltk.corpus import stopwords as nltk_stopwords\nimport spacy\n\n\n# Load the spaCy model\nnlp = spacy.load('en_core_web_sm')\n\n# Preprocessing function\ndef preprocess(text):\n    doc = nlp(text, disable=['ner', 'parser'])\n    tokens = [token for token in doc if token.text.isalpha()]\n    lemmas = [token.lemma_ for token in tokens if token.pos_ not in ['DET', 'NUM', 'SYM', 'X']]\n    #pos = [token.pos_ for token in doc]\n    #print(pos[:10])\n    # Load stopwords from NLTK\n    #stopwords = set(nltk_stopwords.words('english'))\n    # Remove stopwords characters\n    #a_lemmas = [lemma for lemma in lemmas if lemma not in stopwords]\n    \n    return ' '.join(lemmas)\n\n# Preprocessing dataset \nytp = pd.DataFrame(columns=['Comment'])\n\ncomments_list = ytc['Comment'].tolist() \n\nfor i, comment in enumerate(comments_list):\n    if i <= (ytc.shape[0]): #-18000 subtract to limit rows for tests\n        comment = preprocess(comment)\n        ytp = pd.concat([ytp, pd.DataFrame({'Comment': [comment]}, index=[i])])\n\nytp.head()","outputsMetadata":{"0":{"height":244,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"ec2798c6-607e-474a-b66e-75884720c672","nodeType":"const"},"quickFilterText":"","columnVisibility":{"hiddenColIds":[]}}},"1":{"height":610,"type":"stream"}}},"cell_type":"code","id":"ff47fcd5-a9cb-4622-a205-50aa654cb4ea","outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Comment","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2,3,4],"Comment":["let not forget that apple pay in require brand new iphone in order to use it significant portion of apple user base be not able to use it even if they want to as successive iphone incorporate technology and old iphone be replace number of people who could use technology increase","here in nz of retailer do even have contactless credit card machine like paywave support apple pay they do like high fee that come with","I will forever acknowledge channel with help of your lesson and idea explanation now its quite helpful while you ll just sit at your comfort and monitor your account growth","whenever I go to place that do take apple pay do happen too often it drag between contactless covid habit and my get apple card I get so used to apple pay that I get seriously annoy when store do take it it feel like shock it crazy how quickly it take over my shopping routine I officially be brainwash by apple because now it feel so inconvenient to even carry physical card in my pocket","apple pay be so convenient secure and easy to use I use it while at korean and japanese airport need for physical credit card"]}},"total_rows":5,"truncation_type":null},"text/plain":"                                             Comment\n0  let not forget that apple pay in require brand...\n1  here in nz of retailer do even have contactles...\n2  I will forever acknowledge channel with help o...\n3  whenever I go to place that do take apple pay ...\n4  apple pay be so convenient secure and easy to ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>let not forget that apple pay in require brand...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>here in nz of retailer do even have contactles...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I will forever acknowledge channel with help o...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>whenever I go to place that do take apple pay ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>apple pay be so convenient secure and easy to ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":58}],"execution_count":58},{"source":"#1 dataset without preprocessing\n\nX = ytc['Comment'].copy()\ny = ytc['Sentiment'].copy()\n\n#2 dataset with preprocessing\n\nXp = ytp['Comment'].copy()\nyp = ytc['Sentiment'].copy()\n\nsent = {'neutral': 1, 'negative':0, 'positive':2}\nyp= yp.map(sent)\n\n# Ensure the indices of Xp and yp align before concatenation\nXp.index = range(len(Xp))\nyp.index = range(len(yp))\n\nytp_all = pd.concat([Xp, yp], axis=1)\nytp_all.columns = ['Comment', 'Sentiment']\n","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742382654278,"lastExecutedByKernel":"a12c62f9-7958-4dc4-9edd-55951ad98c27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#1 dataset without preprocessing\n\nX = ytc['Comment'].copy()\ny = ytc['Sentiment'].copy()\n\n#2 dataset with preprocessing\n\nXp = ytp['Comment'].copy()\nyp = ytc['Sentiment'].copy()\n\nsent = {'neutral': 1, 'negative':0, 'positive':2}\nyp= yp.map(sent)\n\n# Ensure the indices of Xp and yp align before concatenation\nXp.index = range(len(Xp))\nyp.index = range(len(yp))\n\nytp_all = pd.concat([Xp, yp], axis=1)\nytp_all.columns = ['Comment', 'Sentiment']\n","outputsMetadata":{"0":{"height":38,"type":"dataFrame"},"1":{"height":550,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"ec2798c6-607e-474a-b66e-75884720c672","nodeType":"const"},"quickFilterText":""}}}},"cell_type":"code","id":"28a541cb-e7a5-4d86-935e-2075a98d9185","outputs":[],"execution_count":59},{"source":"#Fit count vectorizer with raw data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\ncv = CountVectorizer(min_df=3, max_df=0.7, ngram_range=(1,2))#min_df=3, ngram_range=(1,2))\ncv_train = cv.fit_transform(X_train.values)\ncv_test = cv.transform(X_test.values)\ncv_train.shape","metadata":{"executionCancelledAt":null,"executionTime":1015,"lastExecutedAt":1742382660120,"lastExecutedByKernel":"a12c62f9-7958-4dc4-9edd-55951ad98c27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Fit count vectorizer with raw data\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n\ncv = CountVectorizer(min_df=3, max_df=0.7, ngram_range=(1,2))#min_df=3, ngram_range=(1,2))\ncv_train = cv.fit_transform(X_train.values)\ncv_test = cv.transform(X_test.values)\ncv_train.shape","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"954c1483-604b-4c63-ba61-777125815263","outputs":[{"output_type":"execute_result","data":{"text/plain":"(14691, 32706)"},"metadata":{},"execution_count":60}],"execution_count":60},{"source":"#Fit count vectorizer with preprocessed data\n\nXp_train, Xp_test, yp_train, yp_test = train_test_split(ytp_all['Comment'], ytp_all['Sentiment'], test_size=0.2, random_state=123, stratify=ytp_all['Sentiment'])\n\ncvp = CountVectorizer(min_df=2, max_df=0.7, ngram_range=(1,2))\ncvp_train = cvp.fit_transform(Xp_train.values)\ncvp_test = cvp.transform(Xp_test.values)\ncvp_train.shape","metadata":{"executionCancelledAt":null,"executionTime":945,"lastExecutedAt":1742382984687,"lastExecutedByKernel":"a12c62f9-7958-4dc4-9edd-55951ad98c27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#Fit count vectorizer with preprocessed data\n\nXp_train, Xp_test, yp_train, yp_test = train_test_split(ytp_all['Comment'], ytp_all['Sentiment'], test_size=0.2, random_state=123, stratify=ytp_all['Sentiment'])\n\ncvp = CountVectorizer(min_df=2, max_df=0.7, ngram_range=(1,2))\ncvp_train = cvp.fit_transform(Xp_train.values)\ncvp_test = cvp.transform(Xp_test.values)\ncvp_train.shape","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"2c949d49-86c8-4423-89e3-1cb8256f63f4","outputs":[{"output_type":"execute_result","data":{"text/plain":"(14691, 50719)"},"metadata":{},"execution_count":83}],"execution_count":83},{"source":"# Compare datasets with multinomial naive bayes classifier model\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nprint('Scores for MultinomialNB:')\n\nmnb = MultinomialNB()\nmnb.fit(cv_train, y_train)\ny_pred = mnb.predict(cv_test)\nprint('Raw dataset: ', round(metrics.accuracy_score(y_test, y_pred), 4))\n# metrics.confusion_matrix(y_test, y_pred)\n\nmnb_cl = MultinomialNB()\nmnb_cl.fit(cvp_train, yp_train)\nyp_pred = mnb_cl.predict(cvp_test)\nprint(f'Preprocessed dataset: {round(metrics.accuracy_score(yp_test, yp_pred), 4)}')\n# metrics.confusion_matrix(y_test, y_pred)\n","metadata":{"executionCancelledAt":null,"executionTime":57,"lastExecutedAt":1742382988168,"lastExecutedByKernel":"a12c62f9-7958-4dc4-9edd-55951ad98c27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Compare datasets with multinomial naive bayes classifier model\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nprint('Scores for MultinomialNB:')\n\nmnb = MultinomialNB()\nmnb.fit(cv_train, y_train)\ny_pred = mnb.predict(cv_test)\nprint('Raw dataset: ', round(metrics.accuracy_score(y_test, y_pred), 4))\n# metrics.confusion_matrix(y_test, y_pred)\n\nmnb_cl = MultinomialNB()\nmnb_cl.fit(cvp_train, yp_train)\nyp_pred = mnb_cl.predict(cvp_test)\nprint(f'Preprocessed dataset: {round(metrics.accuracy_score(yp_test, yp_pred), 4)}')\n# metrics.confusion_matrix(y_test, y_pred)\n","outputsMetadata":{"0":{"height":81,"type":"stream"}}},"cell_type":"code","id":"71e45f3a-373a-412a-a3a1-b67d959fbf82","outputs":[{"output_type":"stream","name":"stdout","text":"Scores for MultinomialNB:\nRaw dataset:  0.7228\nPreprocessed dataset: 0.7343\n"}],"execution_count":84},{"source":"Best scores for MultinomialNB:\n- Raw dataset:  0.7228\n- Preprocessed dataset: 0.7373","metadata":{},"cell_type":"markdown","id":"d962fdda-7235-4244-8978-44b8db7c7a1f"},{"source":"# Compare datasets with support vector machines classifier model\nfrom sklearn.svm import SVC\n\nprint('Scores for SVC:')\n\nsvc = SVC(kernel='linear')\nsvc.fit(cv_train, y_train)\nys_pred = svc.predict(cv_test)\nprint(f'Raw dataset: {round(metrics.accuracy_score(y_test, ys_pred), 4)}')\n\nsvc = SVC(kernel='linear')\nsvc.fit(cvp_train, yp_train)\nyp_pred = svc.predict(cvp_test)\nprint(f'Preprocessed dataset: {round(metrics.accuracy_score(yp_test, yp_pred), 4)}')","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":81,"type":"stream"}}},"cell_type":"code","id":"54cf3279-247f-4807-9ecb-70c2b9b302dd","outputs":[{"output_type":"stream","name":"stdout","text":"Scores for SVC:\nRaw dataset: 0.7329\nPreprocessed dataset: 0.7577\n"}],"execution_count":85},{"source":"Best scores for SVC:\n- Raw dataset: 0.7329\n- Preprocessed dataset: 0.7577","metadata":{},"cell_type":"markdown","id":"50698b03-83d4-4029-a59f-cbf73d49555b"}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}